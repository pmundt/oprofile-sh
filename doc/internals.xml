<?xml version="1.0" encoding='ISO-8859-1'?>
<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V4.1.2//EN" "http://www.oasis-open.org/docbook/xml/4.1.2/docbookx.dtd">

<book id="oprofile-internals">
<bookinfo>
	<title>OProfile Internals</title>
 
	<authorgroup>
		<author>
			<firstname>John</firstname>
			<surname>Levon</surname>
			<affiliation>
				<address><email>levon@movementarian.org</email></address>
			</affiliation>
		</author>
	</authorgroup>

	<copyright>
		<year>2003</year>
		<holder>John Levon</holder>
	</copyright>
</bookinfo>

<toc></toc>

<chapter id="introduction">
<title>Introduction</title>

<para>
This document is current for OProfile version <oprofileversion />.
This document provides some details on the internal workings of OProfile for the
interested hacker. This document assumes strong C, working C++, plus some knowledge of
kernel internals and CPU hardware.
</para>
<note>
<para>
Only the "new" implementation associated with kernel 2.6 and above is covered here. 2.4
uses a very different kernel module implementation and daemon to produce the sample files.
</para>
</note>

<sect1 id="overview">
<title>Overview</title>
<para>
OProfile is a statistical continuous profiler. In other words, profiles are generated by
regularly sampling the current registers on each CPU (from an interrupt handler, the
saved PC value at the time of interrupt is stored), and converting that runtime PC
value into something meaningful to the programmer.
</para>
<para>
OProfile achieves this by taking the stream of sampled PC values, along with the detail
of which task was running at the time of the interrupt, and converting into a file offset
against a particular binary file. Because applications <function>mmap()</function>
the code they run (be it <filename>/bin/bash</filename>, <filename>/lib/libfoo.so</filename>
or whatever), it's possible to find the relevant binary file and offset by walking
the task's list of mapped memory areas. Each PC value is thus converted into a tuple
of binary-image,offset. This is something that the userspace tools can use directly
to reconstruct where the code came from, including the particular assembly instructions,
symbol, and source line (via the binary's debug information if present).
</para>
<para>
Regularly sampling the PC value like this approximates what actually was executed and
how often - more often than not, this statistical approximation is good enough to
reflect reality. In common operation, the time between each sample interrupt is regulated
by a fixed number of clock cycles. This implies that the results will reflect where
the CPU is spending the most time; this is obviously a very useful information source
for performance analysis.
</para>
<para>
Sometimes though, an application programmer needs different kinds of information: for example,
"which of the source routines cause the most cache misses ?". The rise in importance of
such metrics in recent years has led many CPU manufacturers to provide hardware performance
counters capable of measuring these events on the hardware level. Typically, these counters
increment once per each event, and generate an interrupt on reaching some pre-defined
number of events. OProfile can use these interrupts to generate samples: then, the
profile results are a statistical approximation of which code caused how many of the
given event.
</para>
<para>
Consider a simplified system that only executes two functions A and B. A
takes one cycle to execute, whereas B takes 99 cycles. Imagine we run at
100 cycles a second, and we've set the performance counter to create an
interrupt after a set number of "events" (in this case an event is one
clock cycle). It should be clear that the chances of the interrupt
occurring in function A is 1/100, and 99/100 for function B. Thus, we
statistically approximate the actual relative performance features of
the two functions over time. This same analysis works for other types of
events, providing that the interrupt is tied to the number of events
occurring (that is, after N events, an interrupt is generated).
</para>
<para>
There are typically more than one of these counters, so it's possible to set up profiling
for several different event types. Using these counters gives us a powerful, low-overhead
way of gaining performance metrics. If OProfile, or the CPU, does not support performance
counters, then a simpler method is used: the kernel timer interrupt feeds samples
into OProfile itself.
</para>
<para>
The rest of this document concerns itself with how we get from receiving samples at
interrupt time to producing user-readable profile information.
</para>
</sect1>

<sect1 id="components">
<title>Components of the OProfile system</title>

<sect2 id="arch-specific-components">
<title>Architecture-specific components</title>
<para>
If OProfile supports the hardware performance counters found on
a particular architecture, code for managing the details of setting
up and managing these counters can be found in the kernel source
tree in the relevant <filename>arch/<emphasis>arch</emphasis>/oprofile/</filename>
directory. The architecture-specific implementation works via
filling in the oprofile_operations structure at init time. This
provides a set of operations such as <function>setup()</function>,
<function>start()</function>, <function>stop()</function>, etc.
that manage the hardware-specific details of fiddling with the
performance counter registers.
</para>
<para>
The other important facility available to the architecture code is
<function>oprofile_add_sample()</function>.  This is where a particular sample
taken at interrupt time is fed into the generic OProfile driver code.
</para>
</sect2>

<sect2 id="filesystem">
<title>oprofilefs</title>
<para>
OProfile implements a pseudo-filesystem known as "oprofilefs", mounted from
userspace at <filename>/dev/oprofile</filename>. This consists of small
files for reporting and receiving configuration from userspace, as well
as the actual character device that the OProfile userspace receives samples
from. At <function>setup()</function> time, the architecture-specific may
add further configuration files related to the details of the performance
counters. For example, on x86, one numbered directory for each hardware
performance counter is added, with files in each for the event type,
reset value, etc.
</para>
<para>
The filesystem also contains a <filename>stats</filename> directory with
a number of useful counters for various OProfile events.
</para>
</sect2>

<sect2 id="driver">
<title>Generic kernel driver</title>
<para>
This lives in <filename>drivers/oprofile/</filename>, and forms the core of
how OProfile works in the kernel. Its job is to take samples delivered
from the architecture-specific code (via <function>oprofile_add_sample()</function>),
and buffer this data, in a transformed form as described later, until releasing
the data to the userspace daemon via the <filename>/dev/oprofile/buffer</filename>
character device.
</para>
</sect2>

<sect2 id="daemon">
<title>The OProfile daemon</title>
<para>
The OProfile userspace daemon's job is to take the raw data provided by the
kernel and write it to the disk. It takes the single data stream from the
kernel and logs sample data against a number of sample files (found in
<filename>/var/lib/oprofile/samples/current/</filename>. For the benefit
of the "separate" functionality, the names/paths of these sample files
are mangled to reflect where the samples were from: this can include
thread IDs, the binary file path, the event type used, and more.
</para>
<para>
After this final step from interrupt to disk file, the data is now
persistent (that is, changes in the running of the system do not invalidate
stored data). So the post-profiling tools can run on this data at any
time (assuming the original binary files are still available and unchanged,
naturally).
</para>
</sect2>

<sect2 id="post-profiling">
<title>Post-profiling tools</title>
So far, we've collected data, but we've yet to present it in a useful form
to the user. This is the job of the post-profiling tools. In general form,
they collate a subset of the available sample files, load and process each one
correlated against the relevant binary file, and finally produce user-readable
information.
</sect2>

</sect1>

</chapter>

<chapter id="performance-counters">
<title>Performance counter management</title>

<sect1 id ="performance-counters-ui">
<title>Providing a user interface</title>

<para>
The performance counter registers need programming in order to set the
type of event to count, etc. OProfile uses a standard model across all
CPUs for defining these events as follows :
</para>
<informaltable frame="all">
<tgroup cols='2'> 
<tbody>
<row><entry><option>event</option></entry><entry>The event type e.g. DATA_MEM_REFS</entry></row>
<row><entry><option>unit mask</option></entry><entry>The sub-events to count (more detailed specification)</entry></row>
<row><entry><option>counter</option></entry><entry>The hardware counter(s) that can count this event</entry></row>
<row><entry><option>count</option></entry><entry>The reset value (how many events before an interrupt)</entry></row>
<row><entry><option>kernel</option></entry><entry>Whether the counter should increment when in kernel space</entry></row>
<row><entry><option>user</option></entry><entry>Whether the counter should increment when in user space</entry></row>
</tbody>
</tgroup>
</informaltable>
<para>
The term "unit mask" is borrowed from the Intel architectures, and can
further specify exactly when a counter is incremented (for example,
cache-related events can be restricted to particular state transitions
of the cache lines).
</para>
<para>
All of the available hardware events and their details are specified in
the textual files in the <filename>events</filename> directory. The
syntax of these files should be fairly obvious. The user specifies the
names and configuration details of the chosen counters via
<command>opcontrol</command>. These are then written to the kernel
module (in numerical form) via <filename>/dev/oprofile/N/</filename>
where N is the physical hardware counter (some events can only be used
on specific counters; OProfile hides these details from the user when
possible). On IA64, the perfmon-based interface behaves somewhat
differently, as described later.
</para>

</sect1>

<sect1 id="performance-counters-programming">
<title>Programming the performance counter registers</title>

<para>
We have described how the user interface fills in the desired
configuration of the counters and transmits the information to the
kernel. It is the job of the <function>-&gt;setup()</function> method
to actually program the performance counter registers. Clearly, the
details of how this is done is architecture-specific; it is also
model-specific on many architectures. For example, i386 provides methods
for each model type that programs the counter registers correctly
(see the <filename>op_model_*</filename> files in
<filename>arch/i386/oprofile</filename> for the details). The method
reads the values stored in the virtual oprofilefs files and programs
the registers appropriately, ready for starting the actual profiling
session.
</para>
<para>
The architecture-specific drivers make sure to save the old register
settings before doing OProfile setup. They are restored when OProfile
shuts down. This is useful, for example, on i386, where the NMI watchdog
uses the same performance counter registers as OProfile; they cannot
run concurrently, but OProfile makes sure to restore the setup it found
before it was running.
</para>
<para>
In addition to programming the counter registers themselves, other setup
is often necessary. For example, on i386, the local APIC needs
programming in order to make the counter's overflow interrupt appear as
an NMI (non-maskable interrupt). This allows sampling (and therefore
profiling) of regions where "normal" interrupts are masked, enabling
more reliable profiles.
</para>

<sect2 id="performance-counters-start">
<title>Starting and stopping the counters</title>
<para>
Initiating a profiling session is done via writing an ASCII '1'
to the file <filename>/dev/oprofile/enable</filename>. This sets up the
core, and calls into the architecture-specific driver to actually
enable each configured counter. Again, the details of how this is
done is model-specific (for example, the Athlon models can disable
or enable on a per-counter basis, unlike the PPro models).
</para>
</sect2>

<sect2>
<title>IA64 and perfmon</title>
<para>
The IA64 architecture provides a different interface from the other
architectures, using the existing perfmon driver. Register programming
is handled entirely in user-space (see
<filename>daemon/opd_perfmon.c</filename> for the details). A process
is forked for each CPU, which creates a perfmon context and sets the
counter registers appropriately via the
<function>sys_perfmonctl</function> interface. In addition, the actual
initiation and termination of the profiling session is handled via the
same interface using <constant>PFM_START</constant> and
<constant>PFM_STOP</constant>. On IA64, then, there are no oprofilefs
files for the performance counters, as the kernel driver does not
program the registers itself.
</para>
<para>
Instead, the perfmon driver for OProfile simply registers with the
OProfile core with an OProfile-specific UUID. During a profiling
session, the perfmon core calls into the OProfile perfmon driver and
samples are registered with the OProfile core itself as usual (with
<function>oprofile_add_sample()</function>).
</para>
</sect2>

</sect1>

</chapter>

<chapter id="collecting-samples">
<title>Collecting and processing samples</title>

<sect1 id="receiving-interrupts">
<title>Receiving interrupts</title>
<para>
Naturally, how the overflow interrupts are received is specific
to the hardware architecture, unless we are in "timer" mode, where the
logging routine is called directly from the standard kernel timer
interrupt handler.
</para>
<para>
On the i386 architecture, the local APIC is programmed such that when a
counter overflows (that is, it receives an event that causes an integer
overflow of the register value to zero), an NMI is generated. This calls
into the general handler <function>do_nmi()</function>; because OProfile
has registered itself as capable of handling NMI interrupts, this will
call into the OProfile driver code in
<filename>arch/i386/oprofile</filename>. Here, the saved PC value (the
CPU saves the register set at the time of interrupt on the stack
available for inspection) is extracted, and the counters are examined to
find out which one generated the interrupt. Also determined is whether
the system was inside kernel or user space at the time of the interrupt.
These three pieces of information are then forwarded onto the OProfile
core via <function>oprofile_add_sample()</function>. Finally, the
counter values are reset to the chosen count value, to ensure another
interrupt happens after another N events have occurred. Other
architectures behave in a similar manner.
</para>
</sect1>
 
<sect1 id="core-structure">
<title>Core data structures</title>
<para>
Before considering what happens when we log a sample, we shall divert
for a moment and look at the general structure of the data collection
system.
</para>
<para>
OProfile maintains a small buffer for storing the logged samples for
each CPU on the system. Only this buffer is altered when we actually log
a sample (remember, we may still be in an NMI context, so no locking is
possible). The buffer is managed by a two-handed system; the "head"
iterator dictates where the next sample data should be placed in the
buffer. Of course, overflow of the buffer is possible, in which case
the sample is discarded.
</para>
<para>
It is critical to remember that at this point, the PC value is an
absolute value, and is therefore only meaningful in the context of which
task it was logged against. Thus, these per-CPU buffers also maintain
details of which task each logged sample is for, as described in the
next section. In addition, we store whether the sample was in kernel
space or user space (on some architectures and configurations, the address
space is not sub-divided neatly at a specific PC value, so we must store
this information).
</para>
<para>
As well as these small per-CPU buffers, we have a considerably larger
single buffer. This holds the data that is eventually copied out into
the OProfile daemon. On certain system events, the per-CPU buffers are
processed and entered (in mutated form) into the main buffer, known in
the source as the "event buffer". The "tail" iterator indicates the
point from which the CPU may be read, up to the position of the "head"
iterator. This provides an entirely lock-free method for extracting data
from the CPU buffers. This process is described in detail later in this chapter.
</para>
</sect1>

<sect1 id="logging-sample">
<title>Logging a sample</title>
<para>
As mentioned, the sample is logged into the buffer specific to the
current CPU. The CPU buffer is a simple array of pairs of unsigned long
values; for a sample, they hold the PC value and the counter for the
sample. (The counter value is later used to translate back into the relevant
event type the counter was programmed to).
</para>
<para>
In addition to logging the sample itself, we also log task switches.
This is simply done by storing the address of the last task to log a
sample on that CPU in a data structure, and writing a task switch entry
into the buffer if the new value of <function>current()</function> has
changed. Note that later we will directly de-reference this pointer;
this imposes certain restrictions on when and how the CPU buffers need
to be processed.
</para>
<para>
Finally, as mentioned, we log whether we have changed between kernel and
userspace using a similar method. Both of these variables
(<varname>last_task</varname> and <varname>last_is_kernel</varname>) are
reset when the CPU buffer is read.
</para>
</sect1>

<sect1 id="synchronising-buffers">
<title>Synchronising the CPU buffers to the event buffer</title>
<!-- FIXME: update when percpu patch goes in -->
<para>
At some point, we have to process the data in each CPU buffer and enter
it into the main (event) buffer. The file
<filename>buffer_sync.c</filename> contains the relevant code. We
periodically (currently every <constant>HZ</constant>/4 jiffies) start
the synchronisation process. In addition, we process the buffers on
certain events, such as an application calling
<function>munmap()</function>. This is particularly important for
<function>exit()</function> - because the CPU buffers contain pointers
to the task structure, if we don't process all the buffers before the
task is actually destroyed and the task structure freed, then we could
end up trying to dereference a bogus pointer in one of the CPU buffers.
</para>
<para>
We also add a notification when a kernel module is loaded; this is so
that user-space can re-read <filename>/proc/modules</filename> to
determine the load addresses of kernel module text sections. Without
this notification, samples for a newly-loaded module could get lost or
be attributed to the wrong module.
</para>
<para>
The synchronisation itself works in the following manner: first, mutual
exclusion on the event buffer is taken. Remember, we do not need to do
that for each CPU buffer, as we only read from the tail iterator (whilst
interrupts might be arriving at the same buffer, but they will write to
the position of the head iterator, leaving previously written entries
intact). Then, we process each CPU buffer in turn. A CPU switch
notification is added to the buffer first (for
<option>--separate=cpu</option> support). Then the processing of the
actual data starts.
</para>
<para>
As mentioned, the CPU buffer consists of task switch entries and the
actual samples. When the routine <function>sync_buffer()</function> sees
a task switch, the process ID and process group ID are recorded into the
event buffer, along with a dcookie (see below) identifying the
application binary (e.g. <filename>/bin/bash</filename>). The
<varname>mmap_sem</varname> for the task is then taken, to allow safe
iteration across the tasks' list of mapped areas. Each sample is then
processed as described in the next section.
</para>
<para>
After a buffer has been read, the tail iterator is updated to reflect
how much of the buffer was processed. Note that when we determined how
much data there was to read in the CPU buffer, we also called
<function>cpu_buffer_reset()</function> to reset
<varname>last_task</varname> and <varname>last_is_kernel</varname>, as
we've already mentioned. During the processing, more samples may have
been arriving in the CPU buffer; this is OK because we are careful to
only update the tail iterator to how much we actually read - on the next
buffer synchronisation, we will start again from that point.
</para>
</sect1>

<sect1 id="dentry-cookies">
<title>Identifying binary images</title>
<para>
In order to produce useful profiles, we need to be able to associate a
particular PC value sample with an actual ELF binary on the disk. This
leaves us with the problem of how to export this information to
user-space. We create unique IDs that identify a particular directory
entry (dentry), and write those IDs into the event buffer. Later on,
the user-space daemon can call the <function>lookup_dcookie</function>
system call, which looks up the ID and fills in the full path of
the binary image in the buffer user-space passes in. These IDs are
maintained by the code in <filename>fs/dcookies.c</filename>; the
cache lasts for as long as the daemon has the event buffer open.
</para>
</sect1>

<sect1 id="finding-dentry">
<title>Finding a sample's binary image and offset</title>
<para>
We haven't yet described how we process the absolute PC value into
something usable by the user-space daemon. When we find a sample entered
into the CPU buffer, we traverse the list of mappings for the task
(remember, we will have seen a task switch earlier, so we know which
task's lists to look at). When a mapping is found that contains the PC
value, we look up the mapped file's dentry in the dcookie cache. This
gives the dcookie ID that will uniquely identify the mapped file. Then
we alter the absolute value such that it is an offset from the start of
the mapping (the mapping need not start at the start of the actual file,
so we have to consider the offset value of the mapping). These two
values are then entered into the event buffer. In this manner, we have
converted a PC value, which has transitory meaning only, into a static
dcookie/offset tuple for later processing by the daemon.
</para>
<para>
We also attempt to avoid the relatively expensive lookup of the dentry
cookie value by storing the cookie value directly into the dentry
itself; then we can simply derive the cookie value immediately when we
find the correct mapping.
</para>
</sect1>

</chapter>

<chapter id="sample-files">
<title>Generating sample files</title>

<sect1 id="processing-buffer">
<title>Processing the buffer</title>
</sect1>

<sect1 id="sample-file-generation">
<title>Locating and writing sample files</title>
</sect1>

</chapter>

<chapter id="output">
<title>Generating useful output</title>

<sect1 id="profile-specification">
<title>Handling the profile specification</title>
</sect1>

<sect1 id="sample-file-collating">
<title>Collating the candidate sample files</title>
</sect1>

<sect1 id="generating-profile-data">
<title>Generating profile data</title>
</sect1>

<sect1 id="generating-output">
<title>Generating output</title>
</sect1>

</chapter>

<glossary id="glossary">
<title>Glossary of OProfile source concepts and types</title>

<glossentry><glossterm>application image</glossterm>
<glossdef><para>
The primary binary image used by an application. This is derived
from the kernel and corresponds to the binary started upon running
an application: for example, <filename>/bin/bash</filename>.
</para></glossdef></glossentry>

<glossentry><glossterm>binary image</glossterm>
<glossdef><para>
An ELF file containing executable code: this includes kernel modules,
the kernel itself (a.k.a. <filename>vmlinux</filename>), shared libraries,
and application binaries.
</para></glossdef></glossentry>

<glossentry><glossterm>dcookie</glossterm>
<glossdef><para>
Short for "dentry cookie". A unique ID that can be looked up to provide
the full path name of a binary image.
</para></glossdef></glossentry>

<glossentry><glossterm>dependent image</glossterm>
<glossdef><para>
A binary image that is dependent upon an application, used with
per-application separation. Most commonly, shared libraries. For example,
if <filename>/bin/bash</filename> is running and we take
some samples inside the C library itself due to <command>bash</command>
calling library code, then the image <filename>/lib/libc.so</filename>
would be dependent upon <filename>/bin/bash</filename>.
</para></glossdef></glossentry>

<glossentry><glossterm>merging</glossterm>
<glossdef><para>
This refers to the ability to merge several distinct sample files
into one set of data at runtime, in the post-profiling tools. For example,
per-thread sample files can be merged into one set of data, because
they are compatible (i.e. the aggregation of the data is meaningful),
but it's not possible to merge sample files for two different events,
because there would be no useful meaning to the results.
</para></glossdef></glossentry>

<glossentry><glossterm>profile class</glossterm>
<glossdef><para>
A collection of profile data that has been collected under the same
class template. For example, if we're using <command>opreport</command>
to show results after profiling with two performance counters enabled
profiling <constant>DATA_MEM_REFS</constant> and <constant>CPU_CLK_UNHALTED</constant>,
there would be two profile classes, one for each event. Or if we're on
an SMP system and doing per-cpu profiling, and we request
<command>opreport</command> to show results for each CPU side-by-side,
there would be a profile class for each CPU.
</para></glossdef></glossentry>

<glossentry><glossterm>profile specification</glossterm>
<glossdef><para>
The parameters the user passes to the post-profiling tools that limit
what sample files are used. This specification is matched against
the available sample files to generate a selection of profile data.
</para></glossdef></glossentry>

<glossentry><glossterm>profile template</glossterm>
<glossdef><para>
The parameters that define what goes in a particular profile class.
This includes a symbolic name (e.g. "cpu:1") and the code-usable
equivalent.
</para></glossdef></glossentry>

</glossary>

</book>
